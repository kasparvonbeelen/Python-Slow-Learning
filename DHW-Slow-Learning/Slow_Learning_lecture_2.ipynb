{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4.1 - NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the examples below are taken from the [NLTK book](http://www.nltk.org/book/) Before we start, we should install all the required material. Run the cell below to install the tools and corpora. This can take a minute..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below to install the additional material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Python's Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I demonstrate the power of the NLTK by inspecting some of the **prepared corpora** of this library. Later on, I show how you can build your own corpus, and unleash all the nice tools on **your own data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Digital Humanities, we often treat texts as *raw data*, as input for our programs. Interpretations arise from abstraction, for example, by counting word frequencies, analysing specific segments of a corpus (i.e. Key Word In Context, or KWIC analysis) or searching for patterns (i.e. collocations). \n",
    "\n",
    "NLTK provides several tools for both **processing** data and **interpreting** texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what corpora NLTK provides by loading the `book` module from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from nltk.book import *` says as much as \"from NLTK's book module, load all items.\" This loads all the books that are processed in advance for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we discover that NLTK includes the script of 'Monty Python and the Holy Grail'. When we `print` text6 we (surprisingly) can not see the actual content yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a standard procedure, we should uncover the data type of the object we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(text6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first hundred tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text6.tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the text is already properly tokenized (i.e. words and punctuation marks are properly separated from each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do more with this `Text` object. To view all the methods attached to this object, use Python's help function. You can ignore all those that start with a double underscore and scroll down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.text.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the methods attached to the `Text` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.concordance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An oft-used technique for distant reading is **Keyword In Context Analysis** in which we centre a whole corpus on a specific word of interest. NLTK comes with a `concordance()` method that allows you to do just this. For example, how is the word 'grail' used in  'Monty Python and the Holy Grail'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.text.Text.concordance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text6.concordance('grail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more realistic research question would be: how have American presidents used 'democracy' in their Inaugural Addresses since 1861? Try to do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.concordance('democracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the number of hits to print with the `lines` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.concordance('democracy',lines=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about 'monstrous' in Moby Dick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1.concordance('monstrous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "\n",
    "Compare the use of 'love' in Melville's Moby Dick to Jane Austin's Sense and Sensibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.similar()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concordance()` shows words in their context. For example, we saw that monstrous occurred in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size. What other words appear in a **similar context**? We can find out by applying the `.similar` method to an NLTK text and enter the word you want analyse within parentheses (don't forget to put a string between quotation marks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(text1.similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.similarity()` returns a list of semantically related words from one text (based on the intuition that words which share common contexts are related) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text1: Moby Dick by Herman Melville 1851\n",
    "text1.similar(\"monstrous\")\n",
    "print('\\n')\n",
    "#text2: Sense and Sensibility by Jane Austen 1811\n",
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we get **different results for different texts**. Austen uses this word quite differently from Melville; for her, monstrous has positive connotations and sometimes functions as an intensifier like the word very."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text5.similar(\"cool\")\n",
    "print('\\n')\n",
    "text3.similar(\"cool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `common_contexts` allows us to **examine just the contexts** that are shared by two or more words, such as monstrous and very. We have to enclose these words by square brackets as well as parentheses, and separate them with a comma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1.common_contexts([\"monstrous\", \"true\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "\n",
    "- Apply the `.similarities()` and `.common_contexts()` to the Wall Street Journal.\n",
    "\n",
    "- What are the benefits/limits of this tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.dispersion_plot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also determine the **location** of a word in the text: how many words from the beginning it appears. This **positional information** can be displayed using a dispersion plot. Each **stripe** represents an instance of a word, and each **row** represents the entire text. In 1.2 we see some striking patterns of word usage over the last 220 years (in an artificial text constructed by joining the texts of the Inaugural Address Corpus end-to-end). You can produce this plot as shown below. You might like to try more words (e.g., liberty, constitution), and different texts. Can you predict the dispersion of a word before you view it? As before, take care to get the quotes, commas, brackets and parentheses exactly right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "\n",
    "Explore the thematic shifts in Genesis using the dispersion plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `collocations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collocation is a **sequence of words that occur together unusually often**. Thus *red wine* is a collocation, whereas *the wine* is not. A characteristic of collocations is that they are resistant to substitution with words that have similar senses; for example, maroon wine sounds definitely odd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### -- Exercise--\n",
    "\n",
    "- Apply `.collacations` to some of the prepared text. Assess the outcomes and its use for research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we still have not inspected the actual text. NLTK represents texts as a list (an inbuilt data type we encountered earlier). Let's find out where this information is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the index notation for find the first 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text1[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the length of a text from start to finish, in terms of the **words and punctuation** symbols that appear--if you have a closer look at the output of the previous print statement, you'll see that it comprises punctuation marks as individual items. We use the function len to obtain the length of a list, which we'll apply here to the book of Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Is \"Sense and Sensibility\" longer than \"Moby Dick\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(text2) > len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **token** is the technical name for a **sequence of characters** — such as hairy, his, or :) — that we want to **treat as a group** (or **unit** of our analysis). When we count the number of tokens in a text, say, the phrase \"to be or not to be\", we are counting occurrences of these sequences. Thus, in our example phrase there are two occurrences of \"to\", two of \"be\", and one each of \"or\" and \"not\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'to be or not to be'\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are only **four distinct** vocabulary items in this phrase. In Python we can use the `set()` function to count the amount of unique words also called *types*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types = set(tokens)\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise\n",
    "\n",
    "We can apply this to larger texts!\n",
    "\n",
    "- How many tokens does the book of Genesis contain?\n",
    "- How many distinct words does the book of Genesis contain? \n",
    "- Can you print the type-token ratio? What doe you think it means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "Is the type-token ration of Moby Dick greater than Sense and Sensibility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Emotion Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader Sentiment Analyzer\n",
    "The variable paragraphs counts is not the most interesting one, let's have a look at the semtiment values of these mentions of immigration.\n",
    "\n",
    "For this we use **VADER**.\n",
    "\n",
    "[from Github](https://github.com/cjhutto/vaderSentiment): VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool.\n",
    "\n",
    "VADER uses a lexicon (a mapping of words to sentiment values, e.g bad=-1.0, good=+1.0) to compute the sentiment (positivity or negativity) of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to install the vader lexicon first\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the VADER Sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader\n",
    "analyzer = vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can test VADER yourself by changing the value of the ``text`` variable, and running the code block. \n",
    "\n",
    "Can you trick the system? Not very easy isn't it?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Not interesting.\"\n",
    "sentiments_analysis = analyzer.polarity_scores(text)\n",
    "print(sentiments_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested here in the compound, the combination of positive and negative sentiments. We can select this by putting the string 'compound' between square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments_analysis['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Working with your own corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you might wonder: what if I want to investigate *other* texts? Of course, this is possible but requires some *preprocessing* steps. We have to **tokenize** the document on your computer or on the Web (which is just a sequence of characters) to an NLTK `Text` object.\n",
    "\n",
    "Before we do this, let's inspects some of NLTK's preprocessing tools.\n",
    "\n",
    "In the previous lecture, we have already covered a few common preprocessing steps such a removing punctuation and lower casing. Here we will take a slightly different route because NLTK takes cares of many of issues that required these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is useful to process a text by sentence, if we want, for example, inquire the use of different words within its meaningful context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "book = 'This is a sentence. And this another one!'\n",
    "sent_tokenize(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise-- \n",
    "How many sentences does Tolstoy's 'War and Peace' contain (approximately)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "data = requests.get('http://www.gutenberg.org/files/2600/2600-0.txt').text\n",
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now score the sentiment of each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in sent_tokenize(data)[1000:1005]: # we only use a small subset of the sentences\n",
    "    sent_sentiments_analysis = analyzer.polarity_scores(sent)\n",
    "    print(sent)\n",
    "    print(sent_sentiments_analysis['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "\n",
    "Experiment with a snippet from The Guardian:\n",
    "- tokenize by sentence with `sent_tokenize`\n",
    "- compute emotion for each sentence using a `for` loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded to earlier, 'tokens' are the minimal units for the machine to process. We often simply equated this with words, which, in turn, were defined as everything between to whitespaces--but the relationship is more complex. Luckily, NLTK comes with many ready-made tools for splitting strings into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"On the 12th of August, 18-- (just three days after my tenth birthday, when I had been given such wonderful presents), I was awakened at seveno’clock in the morning by Karl Ivanitch slapping the wall close to my head with a fly-flap made of sugar paper and a stick.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not one \"correct\" method for tokenizing texts. Therefore NLTK comes with many different tokenizers. What are their differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(regexp_tokenize(sentence, pattern='\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "\n",
    "- How many tokens are there in 'War and Peace'? How many unique words?\n",
    "- Does the type of tokenization change these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2. Converting data to NLTK Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert your own text to an NLTK corpus, you only need to tokenize the text. Let's download Tolstoy's \"Childhood\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "url = 'http://www.gutenberg.org/files/2142/2142-0.txt'\n",
    "text = requests.get(url).text\n",
    "tokens = word_tokenize(text)\n",
    "nltk_text = nltk.text.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can apply all the NLTK methods to this book! Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "Apply the `concordance`, `similar`, `collocation` and `dispersion_plot` to this book (or another book of your choice, preferably one you are familiar with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do to with texts that are only stored on your own computer. The Python syntax is slightly more complicated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('./texts/katholicismeenfascisme.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_text_2 = nltk.text.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_text_2.concordance('fascisme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Topics: Normalising and Enriching Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming, in its literal sense, amounts to cutting down the branches of a tree to its stem. But also tokens can be reduced to their stem. **Stemming is a crude, rule-based process by which we want to group together different variations of a token.** For example, the word 'eat' will have variations like 'eating', 'eaten', 'eats', and so on. In some applications, when does **not make sense to differentiate between 'eat' and 'eaten'**, we typically use stemming to reduce these grammatical variances to the root of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pst = PorterStemmer()\n",
    "print(pst.stem('loving'))\n",
    "print(pst.stem('loved'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above, we create a stemmer object and apply the `stem()` method to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize('love loved loving flower flowers dogs dog')\n",
    "stemmed = []\n",
    "for token in tokens:\n",
    "    stemmed.append(pst.stem(token))\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "\n",
    "Download the Bible, stem it, and count how often the stem 'love' appears (use the percentage function above). Compare this percentage with a non-stemmed version of the Bible.\n",
    "> Tip: use the count method\n",
    "\n",
    "> l = ['a','a','b','c']\n",
    "\n",
    "> l.count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bible_url = 'http://www.gutenberg.org/cache/epub/10/pg10.txt'\n",
    "\n",
    "text_lower = requests.get(bible_url) # download and lowercase the Bible\n",
    "\n",
    "# complete exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more **methodical** way of converting all the **grammatical/inflected** forms of the root of the word. Lemmatization uses context and **part of speech** (see below) to determine the inflected form of the word and applies **different normalization** rules for each part of speech to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wlem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wlem.lemmatize(\"flowers\",pos='n'))\n",
    "print(wlem.lemmatize(\"was\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wlem.lemmatize(\"run\",pos='v'))\n",
    "print(wlem.lemmatize(\"ran\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pst.stem('run'))\n",
    "print(pst.stem('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wlem.lemmatize(\"mouse\",pos='n'))\n",
    "print(wlem.lemmatize(\"mice\",pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pst.stem('mouse'))\n",
    "print(pst.stem('mice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "tokens = word_tokenize(\"And now for something completely different\")\n",
    "tagged = pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to iterate over a list. The output of `pos_tag` returns a list of **tuples** which we can **unpack** using the following notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_element = tagged[0]\n",
    "print('Tuple = ',first_element)\n",
    "word,tag = first_element\n",
    "print('Word = ',word)\n",
    "print('Tag = ',tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word,tag in tagged:\n",
    "    print('This is a word',word)\n",
    "    print('This is a tag',tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar but slightly more elegant than:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for element in tagged:\n",
    "    print('This is a word',element[0])\n",
    "    print('This is a tag',element[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`. An overview of all the Part-of-Speech tags you'll find [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what does 'RB' mean\n",
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Exercise--\n",
    "\n",
    "Collect all the nouns in \"The Communist Manifesto\" of Marx and Engels.\n",
    "> Tip use the `startswith()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/cache/epub/61/pg61.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = requests.get(url).text # load the text\n",
    "\n",
    "tokens = word_tokenize(text) # tokenize the text\n",
    "\n",
    "pos_tagged = pos_tag(tokens) # part of speech tag the tokenized text, this can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pos_tagged[100:120]) # inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for word,tag in pos_tagged:\n",
    "    #if .startswith ...\n",
    "    # use append\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difficult Exercise**: Collect all [bigrams](https://en.wikipedia.org/wiki/Bigram) (sequence of two words) that start with an adjective and end with a noun.\n",
    "> Tip: Use index notation as shown below (below I apply it to string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also has functions for more refined syntactic analysis and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Mark and John are working at Google.\"\n",
    "ne= ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "ne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
